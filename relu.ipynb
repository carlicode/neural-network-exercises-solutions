{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXtr4Huqfc-d"
      },
      "source": [
        "# ReLU activation function\n",
        "\n",
        "Welcome to this hands-on exercise! ðŸŽ‰  \n",
        "In this notebook, youâ€™ll implement one of the most fundamental components of neural networks: the **Rectified Linear Unit (ReLU)** activation function.  \n",
        "\n",
        "## What are Activation Functions?\n",
        "\n",
        "Activation functions transform the input of a neuron into an output that gets passed to the next layer.  \n",
        "Without them, neural networks would only capture **linear** relationships â€” which isnâ€™t enough for modeling real-world complexity.  \n",
        "\n",
        "Think of it this way:  \n",
        "ðŸ‘‰ Going from **0 â†’ 1 child** changes your banking behavior very differently than going from **3 â†’ 4**. Thatâ€™s non-linearity in action!  \n",
        "\n",
        "---\n",
        "\n",
        "### What is ReLU?\n",
        "\n",
        "The **ReLU** activation function is defined as:\n",
        "\n",
        "$$f(x) = \\max(0, x)$$  \n",
        "\n",
        "- If input > 0 â†’ output = input  \n",
        "- If input â‰¤ 0 â†’ output = 0  \n",
        "\n",
        "Itâ€™s simple, fast, and extremely common in deep learning.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y1JP05DiUkE"
      },
      "source": [
        "### Your Task\n",
        "\n",
        "You need to implement the `relu` function that takes a numpy array as input and returns the ReLU transformation of that array. The function should work for both scalar values and arrays.\n",
        "\n",
        "### Requirements\n",
        "\n",
        "- Your function should handle both positive and negative numbers correctly\n",
        "- It should work with numpy arrays of any shape\n",
        "- The function should be vectorized (no loops needed!)\n",
        "- Make sure to handle edge cases like zero values\n",
        "\n",
        "Letâ€™s get started by setting up our environment ðŸ‘‡"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3v1qFfAfc-f"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set up plotting\n",
        "%matplotlib inline\n",
        "plt.style.use('default')\n",
        "np.random.seed(42)  # For reproducible results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WEjQ5xrfc-g"
      },
      "source": [
        "## Exercise: Implement the ReLU Function\n",
        "\n",
        "Complete the function below. You can use numpy functions to make your implementation efficient and vectorized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJR1tB4vfc-g"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    \"\"\"\n",
        "    Implement the Rectified Linear Unit (ReLU) activation function.\n",
        "\n",
        "    Args:\n",
        "        x: Input array or scalar (numpy array or scalar)\n",
        "\n",
        "    Returns:\n",
        "        numpy array: ReLU transformation of the input\n",
        "\n",
        "    Examples:\n",
        "        >>> relu(3)\n",
        "        3\n",
        "        >>> relu(-2)\n",
        "        0\n",
        "        >>> relu([1, -1, 0, 2])\n",
        "        array([1, 0, 0, 2])\n",
        "    \"\"\"\n",
        "    # Convert input to numpy array if it's not already\n",
        "    x = np.array(x)\n",
        "\n",
        "    # TODO: Implement the ReLU function\n",
        "    # Hint: You can use np.maximum() or np.where()\n",
        "\n",
        "    # Your code goes here\n",
        "    result = #your code goes here\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-QdbhzSfc-g"
      },
      "source": [
        "## Test Your Implementation\n",
        "\n",
        "Let's test your function with some simple examples first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e688Oo3fc-g"
      },
      "outputs": [],
      "source": [
        "# Test with simple values\n",
        "print(\"Testing with simple values:\")\n",
        "print(f\"relu(5) = {relu(5)}\")\n",
        "print(f\"relu(-3) = {relu(-3)}\")\n",
        "print(f\"relu(0) = {relu(0)}\")\n",
        "\n",
        "# Test with arrays\n",
        "test_array = np.array([1, -1, 0, 2, -5, 3])\n",
        "print(f\"\\nInput array: {test_array}\")\n",
        "print(f\"ReLU output: {relu(test_array)}\")\n",
        "\n",
        "# Test with 2D array\n",
        "test_2d = np.array([[-1, 2], [0, -3]])\n",
        "print(f\"\\nInput 2D array:\\n{test_2d}\")\n",
        "print(f\"ReLU output:\\n{relu(test_2d)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh95ikFkfc-g"
      },
      "source": [
        "## Unit Tests\n",
        "\n",
        "Great! Now that weâ€™ve confirmed the basics, letâ€™s run a more rigorous set of unit tests to be completely sure our implementation is solid.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhX1cahIfc-g"
      },
      "outputs": [],
      "source": [
        "def run_unit_tests():\n",
        "    \"\"\"Run comprehensive unit tests for the relu function\"\"\"\n",
        "\n",
        "    print(\"ðŸ§ª Running Unit Tests for ReLU Function\\n\")\n",
        "\n",
        "    # Test 1: Basic functionality\n",
        "    print(\"Test 1: Basic functionality\")\n",
        "    try:\n",
        "        assert relu(5) == 5, f\"Expected 5, got {relu(5)}\"\n",
        "        assert relu(-3) == 0, f\"Expected 0, got {relu(-3)}\"\n",
        "        assert relu(0) == 0, f\"Expected 0, got {relu(0)}\"\n",
        "        print(\"âœ… Passed: Basic functionality works correctly\")\n",
        "    except AssertionError as e:\n",
        "        print(f\"âŒ Failed: {e}\")\n",
        "        return False\n",
        "\n",
        "    # Test 2: Array handling\n",
        "    print(\"\\nTest 2: Array handling\")\n",
        "    try:\n",
        "        test_array = np.array([1, -1, 0, 2, -5, 3])\n",
        "        expected = np.array([1, 0, 0, 2, 0, 3])\n",
        "        result = relu(test_array)\n",
        "        assert np.array_equal(result, expected), f\"Expected {expected}, got {result}\"\n",
        "        print(\"âœ… Passed: Array handling works correctly\")\n",
        "    except AssertionError as e:\n",
        "        print(f\"âŒ Failed: {e}\")\n",
        "        return False\n",
        "\n",
        "    # Test 3: 2D array handling\n",
        "    print(\"\\nTest 3: 2D array handling\")\n",
        "    try:\n",
        "        test_2d = np.array([[-1, 2], [0, -3]])\n",
        "        expected_2d = np.array([[0, 2], [0, 0]])\n",
        "        result_2d = relu(test_2d)\n",
        "        assert np.array_equal(result_2d, expected_2d), f\"Expected {expected_2d}, got {result_2d}\"\n",
        "        print(\"âœ… Passed: 2D array handling works correctly\")\n",
        "    except AssertionError as e:\n",
        "        print(f\"âŒ Failed: {e}\")\n",
        "        return False\n",
        "\n",
        "    # Test 4: Edge cases\n",
        "    print(\"\\nTest 4: Edge cases\")\n",
        "    try:\n",
        "        # Test with very large numbers\n",
        "        assert relu(1e10) == 1e10, f\"Expected 1e10, got {relu(1e10)}\"\n",
        "        assert relu(-1e10) == 0, f\"Expected 0, got {relu(-1e10)}\"\n",
        "\n",
        "        # Test with very small numbers\n",
        "        assert relu(1e-10) == 1e-10, f\"Expected 1e-10, got {relu(1e-10)}\"\n",
        "        assert relu(-1e-10) == 0, f\"Expected 0, got {relu(-1e-10)}\"\n",
        "\n",
        "        # Test with empty array\n",
        "        empty_result = relu(np.array([]))\n",
        "        assert empty_result.size == 0, f\"Expected empty array, got {empty_result}\"\n",
        "\n",
        "        print(\"âœ… Passed: Edge cases handled correctly\")\n",
        "    except AssertionError as e:\n",
        "        print(f\"âŒ Failed: {e}\")\n",
        "        return False\n",
        "\n",
        "    # Test 5: Data type preservation\n",
        "    print(\"\\nTest 5: Data type preservation\")\n",
        "    try:\n",
        "        # Test with float array\n",
        "        float_array = np.array([1.5, -2.7, 0.0])\n",
        "        float_result = relu(float_array)\n",
        "        assert float_result.dtype == float_array.dtype, f\"Expected dtype {float_array.dtype}, got {float_result.dtype}\"\n",
        "\n",
        "        # Test with int array\n",
        "        int_array = np.array([1, -2, 0])\n",
        "        int_result = relu(int_array)\n",
        "        assert int_result.dtype == int_array.dtype, f\"Expected dtype {int_array.dtype}, got {int_result.dtype}\"\n",
        "\n",
        "        print(\"âœ… Passed: Data types preserved correctly\")\n",
        "    except AssertionError as e:\n",
        "        print(f\"âŒ Failed: {e}\")\n",
        "        return False\n",
        "\n",
        "    # Test 6: Mathematical properties\n",
        "    print(\"\\nTest 6: Mathematical properties\")\n",
        "    try:\n",
        "        # Test that ReLU is idempotent (applying it twice gives the same result)\n",
        "        test_values = np.array([-2, -1, 0, 1, 2])\n",
        "        first_apply = relu(test_values)\n",
        "        second_apply = relu(first_apply)\n",
        "        assert np.array_equal(first_apply, second_apply), \"ReLU should be idempotent\"\n",
        "\n",
        "        # Test that ReLU preserves non-negative values\n",
        "        non_neg = np.array([0, 1, 2, 3])\n",
        "        non_neg_result = relu(non_neg)\n",
        "        assert np.array_equal(non_neg, non_neg_result), \"ReLU should preserve non-negative values\"\n",
        "\n",
        "        print(\"âœ… Passed: Mathematical properties verified\")\n",
        "    except AssertionError as e:\n",
        "        print(f\"âŒ Failed: {e}\")\n",
        "        return False\n",
        "\n",
        "    print(\"\\nðŸŽ‰ All tests passed! Your ReLU implementation is correct!\")\n",
        "    return True\n",
        "\n",
        "# Run the tests\n",
        "run_unit_tests()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize the ReLU Function\n",
        "\n",
        "Let's create a visualization to see how your ReLU function behaves:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a range of values to plot\n",
        "x_values = np.linspace(-5, 5, 1000)\n",
        "y_values = relu(x_values)\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_values, y_values, 'b-', linewidth=2, label='ReLU(x)')\n",
        "plt.plot(x_values, x_values, 'r--', alpha=0.5, label='y = x')\n",
        "plt.plot(x_values, np.zeros_like(x_values), 'g--', alpha=0.5, label='y = 0')\n",
        "\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('ReLU(x)')\n",
        "plt.title('ReLU Activation Function')\n",
        "plt.legend()\n",
        "plt.xlim(-5, 5)\n",
        "plt.ylim(-1, 5)\n",
        "plt.show()\n",
        "\n",
        "print(\"If your implementation is correct, you should see:\")\n",
        "print(\"- A line that follows y=x for positive values (blue line)\")\n",
        "print(\"- A line that stays at y=0 for negative values (blue line)\")\n",
        "print(\"- The function should be continuous at x=0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸŽ‰ **Great job!**  \n",
        "Youâ€™ve not only implemented ReLU in NumPy, but youâ€™re now ready to bring that knowledge into **PyTorch**.  \n",
        "This is a big step forward: from coding things manually to using the same tools that power real-world deep learning models.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3YX95Zho0jI"
      },
      "source": [
        "## Next Steps with PyTorch ðŸâš¡\n",
        "\n",
        "Now that youâ€™ve implemented ReLU in NumPy, letâ€™s move on to **PyTorch**, where youâ€™ll use it in real neural networks:\n",
        "\n",
        "- **Use `nn.ReLU`**: practice applying ReLU as a standalone activation layer.  \n",
        "\n",
        "ðŸŽ¯ By doing this, youâ€™ll bridge the gap between theory (NumPy) and practice (PyTorch) â€” the same activation functions you coded manually are the building blocks of deep learning models.\n",
        "\n",
        "---\n",
        "\n",
        "## ReLU Function with PyTorch\n",
        "\n",
        "PyTorch provides ReLU in two flavors:\n",
        "- **Module API:** `nn.ReLU(inplace=False)` â€” use it like a layer (e.g., inside `nn.Sequential`).  \n",
        "- **Functional API:** `torch.nn.functional.relu(x, inplace=False)` â€” call it directly on tensors in `forward`.  \n",
        "\n",
        "[Docs](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n",
        "\n",
        "ðŸ‘‰ In this exercise, weâ€™ll start with the **Module API**.  \n",
        "**Task:** Apply `nn.ReLU()` to a sample tensor.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHlVI0cPo0Pu",
        "outputId": "01b0e887-f751-4678-c025-50ae9044dc4c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# === Part A: Module API ===\n",
        "# TODO 1: Create a ReLU module (do NOT use inplace for now)\n",
        "relu_layer = #your code goes here\n",
        "\n",
        "# Example input tensor\n",
        "x = torch.tensor([[-2.0, -0.5, 0.0, 1.0, 3.0]])\n",
        "print(\"Input:\", x)\n",
        "\n",
        "# TODO 2: Apply ReLU to x using the module\n",
        "out_mod = #your code goes here\n",
        "print(\"Output (nn.ReLU):\", out_mod)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8CYPzb_sUI3"
      },
      "source": [
        "# Unit tests for PyTorch ReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6crnVTtsT5F"
      },
      "outputs": [],
      "source": [
        "# 1) Correct output on the sample tensor\n",
        "expected = torch.tensor([[0., 0., 0., 1., 3.]])\n",
        "assert torch.equal(out_mod, expected), \"âŒ ReLU output is incorrect on the sample input.\"\n",
        "\n",
        "# 2) Non-negativity check\n",
        "assert torch.all(out_mod >= 0), \"âŒ ReLU produced negative values.\"\n",
        "\n",
        "# 3) Shape preservation\n",
        "assert out_mod.shape == x.shape, \"âŒ ReLU changed the shape of the tensor.\"\n",
        "\n",
        "# 4) Random tensor test (reproducible)\n",
        "torch.manual_seed(0)\n",
        "rand_x = torch.randn(5, 5)\n",
        "rand_out = relu_layer(rand_x)\n",
        "assert torch.all(rand_out >= 0), \"âŒ ReLU produced negatives on random input.\"\n",
        "assert rand_out.shape == rand_x.shape, \"âŒ Output shape mismatch on random test.\"\n",
        "\n",
        "print(\"âœ… All ReLU PyTorch tests passed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ðŸŽ‰ Fantastic work!  \n",
        "Youâ€™ve now implemented and validated ReLU both in **NumPy** and **PyTorch**.  \n",
        "This means youâ€™re not only coding activation functions from scratch, but also using them inside one of the most popular deep learning frameworks today. ðŸš€\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7Ww6UDwfc-h"
      },
      "source": [
        "---\n",
        "\n",
        "## SOLUTION (NOT FOR LEARNERS)\n",
        "\n",
        "**Note: This section contains the complete solution and should not be shown to learners.**\n",
        "\n",
        "Here's the complete implementation of the ReLU function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g86zO1mgfc-h"
      },
      "outputs": [],
      "source": [
        "# SOLUTION: Complete ReLU implementation\n",
        "def relu_solution(x):\n",
        "    \"\"\"\n",
        "    Complete solution for the ReLU activation function.\n",
        "\n",
        "    Args:\n",
        "        x: Input array or scalar (numpy array or scalar)\n",
        "\n",
        "    Returns:\n",
        "        numpy array: ReLU transformation of the input\n",
        "    \"\"\"\n",
        "    # Convert input to numpy array if it's not already\n",
        "    x = np.array(x)\n",
        "\n",
        "    # Method 1: Using np.maximum (most common and efficient)\n",
        "    result = np.maximum(0, x)\n",
        "\n",
        "    # Alternative methods:\n",
        "    # Method 2: Using np.where\n",
        "    # result = np.where(x > 0, x, 0)\n",
        "\n",
        "    # Method 3: Using boolean indexing\n",
        "    # result = x.copy()\n",
        "    # result[x < 0] = 0\n",
        "\n",
        "    return result\n",
        "\n",
        "print(\"Solution implementation:\")\n",
        "print(\"def relu(x):\")\n",
        "print(\"    x = np.array(x)\")\n",
        "print(\"    return np.maximum(0, x)\")\n",
        "\n",
        "print(\"\\nKey points about this solution:\")\n",
        "print(\"1. np.maximum(0, x) is the most efficient and readable way\")\n",
        "print(\"2. It automatically handles broadcasting for arrays of different shapes\")\n",
        "print(\"3. It preserves the data type of the input\")\n",
        "print(\"4. It's vectorized, so it's fast for large arrays\")\n",
        "print(\"5. It correctly handles all edge cases (zeros, negative numbers, etc.)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eox6qAb6rs2D"
      },
      "source": [
        "Here's the complete implementation of the ReLU with pytorch function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jkr9Z7cCrrrX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# === Part A: Module API ===\n",
        "# TODO 1: Create a ReLU module (do NOT use inplace for now)\n",
        "relu_layer = nn.ReLU()\n",
        "\n",
        "# Example input tensor\n",
        "x = torch.tensor([[-2.0, -0.5, 0.0, 1.0, 3.0]])\n",
        "print(\"Input:\", x)\n",
        "\n",
        "# TODO 2: Apply ReLU to x using the module\n",
        "out_mod = relu_layer(x)\n",
        "print(\"Output (nn.ReLU):\", out_mod)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "autopilot",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
