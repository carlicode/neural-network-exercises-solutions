{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing a 3-Layer Neural Network with PyTorch\n",
    "\n",
    "Welcome! In this notebook, we will explore how to **initialize the parameters of a simple 3-layer neural network** using PyTorch.  \n",
    "Rather than starting with a large dataset like MNIST, we will focus on a **Mini Iris** dataset — a small, intuitive problem where we classify flowers based on just a few physical measurements.  \n",
    "\n",
    "### Why is initialization important?\n",
    "Proper initialization of weights and biases is crucial to:\n",
    "- Ensure stable gradients during training.  \n",
    "- Speed up convergence.  \n",
    "- Avoid issues like vanishing or exploding activations.  \n",
    "\n",
    "### What you will learn\n",
    "By the end of this notebook, you will be able to:\n",
    "- Build a **3-layer feedforward neural network** in PyTorch (Input → Hidden → Output).  \n",
    "- Apply **Xavier initialization** to the weights and zero initialization to the biases.  \n",
    "- Train and evaluate the model on the **Mini Iris dataset**.  \n",
    "- Validate your work with **unit tests** that check shapes, initialization, and learning performance.  \n",
    "\n",
    "Let’s get started 🚀\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✅ All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Initialize a 3-Layer Neural Network\n",
    "\n",
    "In this exercise, you will translate the high-level idea of a 3-layer neural network into an actual **PyTorch implementation**.  \n",
    "Your main goal is to focus on **building the architecture** and **applying Xavier initialization** to the parameters.  \n",
    "\n",
    "### What to do:\n",
    "- Define the network using `nn.Sequential`.  \n",
    "- Add a **linear layer** that maps the 3 input features to 4 hidden neurons.  \n",
    "- Insert a **ReLU activation** for non-linearity.  \n",
    "- Add a second **linear layer** that maps 4 hidden neurons to 2 output classes.  \n",
    "- Initialize all weights with **Xavier initialization** and set all biases to zero.  \n",
    "\n",
    "👉 Don’t worry about training yet — for now, just make sure the network is built and initialized correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the PyTorch implementation below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def initialize_3layer_network(n_inputs=3, n_hidden=4, n_outputs=2):\n",
    "    \"\"\"\n",
    "    TODO: Initialize a simple 3-layer neural network in PyTorch.\n",
    "\n",
    "    The network should have:\n",
    "    - Input -> Hidden (Linear)\n",
    "    - ReLU activation\n",
    "    - Hidden -> Output (Linear)\n",
    "\n",
    "    Then apply Xavier initialization to all linear layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Create a PyTorch Sequential model\n",
    "    # Hint: Use nn.Linear layers with correct dimensions\n",
    "    # Example: nn.Linear(input_dim, output_dim)\n",
    "    # TODO: Replace ... with proper dimensions\n",
    "    model = nn.Sequential(...)\n",
    "    \n",
    "    # TODO: Apply Xavier initialization\n",
    "    # Hint: Use nn.init.xavier_normal_() for weights\n",
    "    #       Use nn.init.zeros_() for biases\n",
    "    for layer in model:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            # YOUR CODE HERE\n",
    "            pass\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Iris Dataset Preparation\n",
    "\n",
    "In this section, we will prepare a simplified version of the Iris dataset to train our 3-layer neural network.\n",
    "\n",
    "We select 3 input features (sepal length, sepal width, and petal length) to match our network’s input size.\n",
    "\n",
    "We restrict the dataset to only two flower species (Setosa and Versicolor), since our output layer has 2 neurons (binary classification).\n",
    "\n",
    "The data is then converted into PyTorch tensors and split into training (80%) and testing (20%) sets.\n",
    "\n",
    "This setup ensures that the dataset structure is consistent with our model:\n",
    "Input layer (3 features) → Hidden layer (4 neurons) → Output layer (2 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the classic Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Select 3 input features to match our network architecture\n",
    "# Here: sepal length, sepal width, petal length\n",
    "X = iris['data'][:, [0, 1, 2]]\n",
    "y = iris['target']\n",
    "\n",
    "# Filter only two classes (Setosa=0, Versicolor=1)\n",
    "# This converts the problem into binary classification\n",
    "mask = (y < 2)\n",
    "X, y = X[mask], y[mask]\n",
    "\n",
    "# Convert data into PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Shuffle the dataset randomly\n",
    "perm = torch.randperm(len(X))\n",
    "X, y = X[perm], y[perm]\n",
    "\n",
    "# Split into training (80%) and testing (20%) sets\n",
    "split = int(0.8 * len(X))\n",
    "X_train, y_train = X[:split], y[:split]\n",
    "X_test, y_test   = X[split:], y[split:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization and Training Setup\n",
    "\n",
    "Now that we have prepared the dataset, let’s initialize our 3-layer neural network and set up the training components.  \n",
    "\n",
    "- **Model**: We call `initialize_3layer_network(3, 4, 2)` to create a network with 3 input features, 4 hidden neurons, and 2 output classes.  \n",
    "- **Loss function**: We use `CrossEntropyLoss`, which is standard for classification tasks.  \n",
    "- **Optimizer**: We use `Adam` with a learning rate of `0.03` to update the model parameters efficiently.  \n",
    "\n",
    "This setup will allow us to train the network on the Mini Iris dataset in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize model\n",
    "model = initialize_3layer_network(n_inputs=3, n_hidden=4, n_outputs=2)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Neural Network\n",
    "\n",
    "Now we are ready to train our 3-layer neural network on the Mini Iris dataset.  \n",
    "\n",
    "The training process consists of repeating the following steps for several epochs:  \n",
    "1. **Forward pass**: Pass the input data through the model to compute predictions (logits).  \n",
    "2. **Loss computation**: Compare predictions with the true labels using the loss function.  \n",
    "3. **Backward pass**: Compute gradients of the loss with respect to model parameters.  \n",
    "4. **Parameter update**: Use the optimizer (Adam) to update the weights and biases.  \n",
    "\n",
    "We will train for **300 epochs**, printing the loss every 50 epochs to monitor progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    logits = model(X_train)\n",
    "    loss = criterion(logits, y_train)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print progress every 50 epochs\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "After training the network, we need to evaluate how well it performs on both the **training** and **test** sets.  \n",
    "\n",
    "- We define an `accuracy` function that compares the model’s predictions with the true labels.  \n",
    "- Accuracy is computed as the proportion of correctly classified samples.  \n",
    "- We will report results on both the training set (to check learning) and the test set (to check generalization).  \n",
    "\n",
    "This gives us a good indication of whether our 3-layer neural network has successfully learned to separate the two flower classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, X, y):\n",
    "    with torch.no_grad():\n",
    "        preds = model(X).argmax(1)\n",
    "        return (preds == y).float().mean().item()\n",
    "\n",
    "print(\"Train Accuracy:\", round(accuracy(model, X_train, y_train), 3))\n",
    "print(\"Test Accuracy:\", round(accuracy(model, X_test, y_test), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Sample Predictions\n",
    "\n",
    "To better understand how our model works, let’s look at a few predictions on unseen test samples:  \n",
    "\n",
    "- We will take the first **5 samples** from the test set.  \n",
    "- The model will output predicted class labels.  \n",
    "- We will compare these predictions with the **true labels**.  \n",
    "\n",
    "This helps us verify that the network is not only achieving good accuracy, but also making correct predictions on individual examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sample = X_test[:5]\n",
    "    preds = model(sample).argmax(1)\n",
    "    print(\"Sample predictions:\", preds.tolist())\n",
    "    print(\"True labels:\", y_test[:5].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Sample Predictions\n",
    "\n",
    "In this output, we compared the model’s predictions against the true labels for the first **5 samples** in the test set.\n",
    "\n",
    "- **Predictions:** `[0, 0, 0, 0, 1]`  \n",
    "- **True labels:** `[0, 0, 0, 0, 1]`  \n",
    "\n",
    "The model correctly classified **all five samples**, meaning its decision boundaries generalize well to unseen data.  \n",
    "This small test is not a substitute for the full accuracy evaluation, but it provides a quick qualitative check that the model is not only memorizing the training data, but also making correct predictions on new examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Tests with Explicit Feedback\n",
    "\n",
    "Run the following tests to verify your implementation. Each test provides clear feedback if something is incorrect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Unit Tests for the Mini Iris pipeline (init → train → eval)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def _clone_params(m):\n",
    "    return [p.detach().clone() for p in m.parameters()]\n",
    "\n",
    "# --- Build model and training objects ---\n",
    "torch.manual_seed(0)\n",
    "model = initialize_3layer_network(n_inputs=3, n_hidden=4, n_outputs=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.03)\n",
    "\n",
    "# 1) Initialization checks (layers, shapes, zeros in bias, sane variance)\n",
    "assert isinstance(model[0], nn.Linear) and isinstance(model[2], nn.Linear), \\\n",
    "    \"❌ Expected Linear layers at positions [0] and [2].\"\n",
    "\n",
    "W1, b1 = model[0].weight.detach(), model[0].bias.detach()\n",
    "W2, b2 = model[2].weight.detach(), model[2].bias.detach()\n",
    "\n",
    "assert W1.shape == (4, 3), f\"❌ W1 shape {tuple(W1.shape)}; expected (4, 3).\"\n",
    "assert W2.shape == (2, 4), f\"❌ W2 shape {tuple(W2.shape)}; expected (2, 4).\"\n",
    "assert torch.allclose(b1, torch.zeros_like(b1)), \"❌ b1 is not zero-initialized.\"\n",
    "assert torch.allclose(b2, torch.zeros_like(b2)), \"❌ b2 is not zero-initialized.\"\n",
    "\n",
    "v1, v2 = W1.var().item(), W2.var().item()\n",
    "assert 1e-3 < v1 < 1.0, f\"❌ Var(W1)={v1:.4f} out of reasonable range; check Xavier init.\"\n",
    "assert 1e-3 < v2 < 1.0, f\"❌ Var(W2)={v2:.4f} out of reasonable range; check Xavier init.\"\n",
    "\n",
    "# 2) Forward pass shape\n",
    "logits = model(X_train)\n",
    "assert logits.shape == (X_train.shape[0], 2), \\\n",
    "    f\"❌ Forward shape {tuple(logits.shape)}; expected ({X_train.shape[0]}, 2).\"\n",
    "\n",
    "# 3) Parameters should update after one optimizer step\n",
    "before = _clone_params(model)\n",
    "loss = criterion(logits, y_train)\n",
    "optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "after = [p.detach().clone() for p in model.parameters()]\n",
    "assert any(not torch.allclose(b, a, atol=1e-7) for b, a in zip(before, after)), \\\n",
    "    \"❌ Parameters did not update after an optimizer step.\"\n",
    "\n",
    "# 4) Training should reduce loss meaningfully\n",
    "def eval_loss(m, X, y):\n",
    "    with torch.no_grad():\n",
    "        return criterion(m(X), y).item()\n",
    "\n",
    "loss_before = eval_loss(model, X_train, y_train)\n",
    "for _ in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(X_train)\n",
    "    l = criterion(out, y_train)\n",
    "    l.backward(); optimizer.step()\n",
    "loss_after = eval_loss(model, X_train, y_train)\n",
    "\n",
    "assert (loss_after < loss_before * 0.7) or (loss_after < 0.25), \\\n",
    "    f\"❌ Loss did not decrease enough: before={loss_before:.4f}, after={loss_after:.4f}.\"\n",
    "\n",
    "# 5) Test accuracy threshold (binary Mini Iris should be high)\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test).argmax(1)\n",
    "    acc_test = (preds == y_test).float().mean().item()\n",
    "\n",
    "assert acc_test >= 0.85, f\"❌ Test accuracy too low: {acc_test:.3f} (expected ≥ 0.85).\"\n",
    "\n",
    "print(f\"✅ All tests passed! (loss: {loss_before:.4f} → {loss_after:.4f}, test_acc={acc_test:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations!\n",
    "\n",
    "All tests have passed successfully ✅  \n",
    "You have:\n",
    "\n",
    "- Correctly initialized a **3-layer neural network** with PyTorch.  \n",
    "- Trained it on the **Mini Iris dataset**.  \n",
    "- Evaluated its performance with accuracy and sample predictions.  \n",
    "\n",
    "This means your model is not only running without errors, but it’s also **learning to separate flower classes effectively** 🌸🚀  \n",
    "\n",
    "Keep going — you just leveled up your PyTorch skills! 🔥\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## INSTRUCTOR SOLUTION (HIDDEN FROM STUDENTS) \n",
    "\n",
    "**Note: This section is for instructors only and should not be visible to learners.**\n",
    "\n",
    "Below is the complete solution to the exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ INSTRUCTOR SOLUTION (HIDDEN FROM STUDENTS)\n",
    "def initialize_3layer_network(n_inputs=3, n_hidden=4, n_outputs=2):\n",
    "    \"\"\"\n",
    "    SOLUTION: Initialize parameters for a 3-layer neural network using PyTorch.\n",
    "    \n",
    "    Args:\n",
    "        n_inputs (int): Number of input features (default: 3)\n",
    "        n_hidden (int): Number of hidden neurons (default: 4)\n",
    "        n_outputs (int): Number of output classes (default: 2)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing initialized parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a PyTorch Sequential model\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(n_inputs, n_hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(n_hidden, n_outputs)\n",
    "    )\n",
    "    \n",
    "    # Apply Xavier initialization to all linear layers\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_normal_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "    \n",
    "    # Extract parameters and convert to dictionary format\n",
    "    parameters = {}\n",
    "    parameters['W1'] = model[0].weight.data\n",
    "    parameters['b1'] = model[0].bias.data.unsqueeze(1)  # Make it (n_hidden, 1)\n",
    "    parameters['W2'] = model[2].weight.data\n",
    "    parameters['b2'] = model[2].bias.data.unsqueeze(1)  # Make it (n_outputs, 1)\n",
    "    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autopilot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
